{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "70c39cba89a1457c99a975ea0eff84e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3db0593f03e493b9a8abb47a887cc42",
              "IPY_MODEL_3502715e99b8469fb355a160c6ef369e",
              "IPY_MODEL_0706a6b0fe5a48ba9b5fa991f572d348"
            ],
            "layout": "IPY_MODEL_b855adae31a5430f87c2404425898692"
          }
        },
        "a3db0593f03e493b9a8abb47a887cc42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fea1eabc807e4ffaaac725fbcd9048cc",
            "placeholder": "​",
            "style": "IPY_MODEL_a76f0fede8d7485485b0907ec01c96ca",
            "value": "Map: 100%"
          }
        },
        "3502715e99b8469fb355a160c6ef369e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cecb0ef215994b9eb73a6ae34ee0ff10",
            "max": 59199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab5676c26e88455e90ac3e48ef63a8e0",
            "value": 59199
          }
        },
        "0706a6b0fe5a48ba9b5fa991f572d348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cc534bab6244d8aa15ba4da80324ab7",
            "placeholder": "​",
            "style": "IPY_MODEL_819f9cc532f341c89731a15347a7399b",
            "value": " 59199/59199 [04:03&lt;00:00, 272.73 examples/s]"
          }
        },
        "b855adae31a5430f87c2404425898692": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fea1eabc807e4ffaaac725fbcd9048cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a76f0fede8d7485485b0907ec01c96ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cecb0ef215994b9eb73a6ae34ee0ff10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab5676c26e88455e90ac3e48ef63a8e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cc534bab6244d8aa15ba4da80324ab7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "819f9cc532f341c89731a15347a7399b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc80c2bb943a4f9e973520fe575551b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04a05cf5821f4766a4ec958c1ba2d67c",
              "IPY_MODEL_7c655d93aac3403a9586eabfd1e36b54",
              "IPY_MODEL_92f6ef16835541948243367bd5d3432e"
            ],
            "layout": "IPY_MODEL_79293a71f5484733a54623e75742cc99"
          }
        },
        "04a05cf5821f4766a4ec958c1ba2d67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a21177693a0144f4ada65d70860a9469",
            "placeholder": "​",
            "style": "IPY_MODEL_64a2b9a8d0d6441997337521f366f60b",
            "value": "Map: 100%"
          }
        },
        "7c655d93aac3403a9586eabfd1e36b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c380ea0d07644875a896f7768a15ca6e",
            "max": 598,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc6e9723aae84edf9fe913a6098e93cf",
            "value": 598
          }
        },
        "92f6ef16835541948243367bd5d3432e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6d0b8b9d4f04496a08a0afcb47bbd0c",
            "placeholder": "​",
            "style": "IPY_MODEL_137276d11dd94d9ba110c94eb27838dd",
            "value": " 598/598 [00:02&lt;00:00, 302.94 examples/s]"
          }
        },
        "79293a71f5484733a54623e75742cc99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a21177693a0144f4ada65d70860a9469": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64a2b9a8d0d6441997337521f366f60b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c380ea0d07644875a896f7768a15ca6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc6e9723aae84edf9fe913a6098e93cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6d0b8b9d4f04496a08a0afcb47bbd0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "137276d11dd94d9ba110c94eb27838dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXPlPOpbn8X7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4520d52f-a3e7-4e35-8b50-9a6e41703be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on GPU: True\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes sacremoses fsspec==2025.3.2\n",
        "\n",
        "# Check that a GPU is available (for faster training and 8-bit quantization)\n",
        "import torch\n",
        "print(\"Running on GPU:\", torch.cuda.is_available())\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/biogpt_lora_model\"  # You can rename as needed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the JSONL file manually into a list of dicts\n",
        "jsonl_path = \"/content/mimic_cxr_finetune.jsonl\"\n",
        "data = []\n",
        "with open(jsonl_path, 'r') as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "# Create a HuggingFace Dataset from the list\n",
        "raw_dataset = Dataset.from_list(data)\n",
        "\n",
        "# Split into train and validation sets (e.g., 99% train, 1% eval)\n",
        "split_datasets = raw_dataset.train_test_split(test_size=0.01, seed=42)\n",
        "train_dataset = split_datasets[\"train\"]\n",
        "eval_dataset = split_datasets[\"test\"]\n",
        "\n",
        "# Print example\n",
        "print(train_dataset[0])\n",
        "print(f\"Train samples: {len(train_dataset)}; Validation samples: {len(eval_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7-MURj4r2Y-",
        "outputId": "75406dfa-0bc5-41d3-dbb5-eb32f48c047d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': 'Atelectasis: uncertain; Cardiomegaly: 1; Consolidation: uncertain; Edema: 1; Pleural Effusion: 1. \\nFindings:', 'completion': \" ET and NG tube are nominal in position.\\n \\n Cardiomegaly could be slightly more pronounced.\\n \\n Again seen are bilateral right greater left effusions with underlying collapse\\n and/or consolidation.\\n \\n Also again seen is CHF, with upper zone redistribution, vascular plethora and\\n probable vascular blurring.  On today's exam, there is increased obscuration\\n of the right hemidiaphragm, which could reflect increased collapse/\\n consolidation at the right base.  Biapical pleural scarring, with surface\\n calcification again noted.  There is also nearby vascular calcification.\\nImpression: Question interval increase in degree of cardiomegaly.\\n \\n CHF and bilateral effusions again seen.\\n \\n Bibasilar collapse and/or consolidation.  This is likely worse on the right,\\n where the right hemidiaphragm is now obscured.\", 'study_id': 52711754, 'subject_id': 11512225}\n",
            "Train samples: 59199; Validation samples: 598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Choose the model checkpoint and tokenizer.\n",
        "model_name = \"microsoft/biogpt\"  # BioGPT (biomedical GPT-2 like model)\n",
        "# If BioGPT is not suitable or unavailable, you could use a general model:\n",
        "# model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "# For GPT models, the pad_token might be missing by default. We set it to the EOS token to avoid errors.\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Define the maximum sequence length\n",
        "MAX_LEN = 512\n",
        "\n",
        "def preprocess_function(example):\n",
        "    prompt = example[\"prompt\"]\n",
        "    completion = example[\"completion\"]\n",
        "    # Combine prompt and completion with a newline separator.\n",
        "    # (The newline helps the model distinguish prompt from report text.)\n",
        "    full_text = prompt + \"\\n\" + completion\n",
        "\n",
        "    # Tokenize the combined text. We use truncation and padding to MAX_LEN.\n",
        "    tokens = tokenizer(full_text, max_length=MAX_LEN, truncation=True, padding=\"max_length\")\n",
        "    input_ids = tokens[\"input_ids\"]\n",
        "    attention_mask = tokens[\"attention_mask\"]\n",
        "\n",
        "    # Prepare labels: copy of input_ids\n",
        "    labels = input_ids.copy()\n",
        "    # Determine prompt token length (including the trailing newline as part of prompt).\n",
        "    # We tokenize the prompt with a newline to get the token count for prompt segment.\n",
        "    prompt_tokens = tokenizer(prompt + \"\\n\", add_special_tokens=False).input_ids\n",
        "    prompt_length = len(prompt_tokens)\n",
        "    if prompt_length > MAX_LEN:\n",
        "        prompt_length = MAX_LEN  # edge case: if prompt itself is longer than MAX_LEN\n",
        "\n",
        "    # Mask out the prompt tokens in the labels\n",
        "    for i in range(min(prompt_length, len(labels))):\n",
        "        labels[i] = -100  # -100 will mask this token from loss computation\n",
        "\n",
        "    # Also mask padding tokens in the labels\n",
        "    # (If pad_token_id equals eos_token_id, we still don’t want to compute loss on padding.)\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    labels = [(-100 if token_id == pad_token_id else token_id) for token_id in labels]\n",
        "\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "# Apply the preprocessing to the training and validation datasets\n",
        "tokenized_train = train_dataset.map(preprocess_function, batched=False, remove_columns=train_dataset.column_names)\n",
        "tokenized_eval = eval_dataset.map(preprocess_function, batched=False, remove_columns=eval_dataset.column_names)\n",
        "print(\"Tokenization sample:\", tokenized_train[0][\"input_ids\"][:10], \"...\\nCorresponding labels:\", tokenized_train[0][\"labels\"][:10], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "70c39cba89a1457c99a975ea0eff84e1",
            "a3db0593f03e493b9a8abb47a887cc42",
            "3502715e99b8469fb355a160c6ef369e",
            "0706a6b0fe5a48ba9b5fa991f572d348",
            "b855adae31a5430f87c2404425898692",
            "fea1eabc807e4ffaaac725fbcd9048cc",
            "a76f0fede8d7485485b0907ec01c96ca",
            "cecb0ef215994b9eb73a6ae34ee0ff10",
            "ab5676c26e88455e90ac3e48ef63a8e0",
            "6cc534bab6244d8aa15ba4da80324ab7",
            "819f9cc532f341c89731a15347a7399b",
            "cc80c2bb943a4f9e973520fe575551b4",
            "04a05cf5821f4766a4ec958c1ba2d67c",
            "7c655d93aac3403a9586eabfd1e36b54",
            "92f6ef16835541948243367bd5d3432e",
            "79293a71f5484733a54623e75742cc99",
            "a21177693a0144f4ada65d70860a9469",
            "64a2b9a8d0d6441997337521f366f60b",
            "c380ea0d07644875a896f7768a15ca6e",
            "dc6e9723aae84edf9fe913a6098e93cf",
            "f6d0b8b9d4f04496a08a0afcb47bbd0c",
            "137276d11dd94d9ba110c94eb27838dd"
          ]
        },
        "id": "vTF84g3nvWyD",
        "outputId": "9e948678-8d78-4926-d5a4-0012952aca41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/59199 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70c39cba89a1457c99a975ea0eff84e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/598 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc80c2bb943a4f9e973520fe575551b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization sample: [2, 5619, 12805, 38946, 20, 6625, 44, 15489, 17921, 20] ...\n",
            "Corresponding labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall -y bitsandbytes\n",
        "# !pip install git+https://github.com/jllllll/bitsandbytes-wheels.git@main\n",
        "\n",
        "!pip install -U bitsandbytes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hc8DVrO4fRJ",
        "outputId": "6d82c820-81c6-44d3-fbda-73d9e7377ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate peft\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX5Zr44t5pxn",
        "outputId": "4a100d19-f510-421e-bff5-6e64070f1e1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "\n",
        "model_name = \"microsoft/biogpt\"\n",
        "\n",
        "# Load the pre-trained base model in 8-bit mode.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,    # load weights in int8 precision\n",
        "    device_map=\"auto\"     # let accelerate auto-distribute the model on available devices (GPU)\n",
        ")\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_8bit=True,\n",
        "#     llm_int8_threshold=6.0,\n",
        "#     llm_int8_has_fp16_weight=True\n",
        "# )\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_name,\n",
        "#     quantization_config=bnb_config,\n",
        "#     device_map=\"auto\"\n",
        "# )\n",
        "\n",
        "# Prepare model for 8-bit training (this enables gradient checkpointing and other tweaks for int8).\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Define LoRA configuration for causal LM\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # LoRA will be applied to Query and Value projection matrices\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False  # we are setting up for training\n",
        ")\n",
        "\n",
        "# Attach LoRA adapters to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# (Optional) Print the number of trainable parameters to verify LoRA is applied\n",
        "trainable_params = 0\n",
        "total_params = 0\n",
        "for param in model.parameters():\n",
        "    total_params += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "print(f\"Trainable params: {trainable_params} (~{100 * trainable_params/ total_params:.2f}% of total)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K998Y0eNxSrb",
        "outputId": "fe716c22-8b2b-4780-ed30-12d76b3be2eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 786432 (~0.23% of total)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Testing for small dataset\n",
        "# small_train_dataset = tokenized_train.shuffle(seed=42).select(range(2000))\n",
        "# small_eval_dataset = tokenized_eval.shuffle(seed=42).select(range(100))\n"
      ],
      "metadata": {
        "id": "Dt1N6Z122oHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"radiology_report_lora\",   # output directory for model checkpoints\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,        # accumulate gradients to simulate batch_size = 16\n",
        "    num_train_epochs=2,                   # train for 2 epochs (can adjust to 1 or more as needed)\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,                            # enable mixed precision training\n",
        "    logging_steps=500,                    # log training metrics every 500 steps\n",
        "    eval_strategy=\"epoch\",          # evaluate on the validation set at end of each epoch\n",
        "    save_strategy=\"epoch\",                # save model at end of each epoch\n",
        "    save_total_limit=1,                   # only keep the most recent checkpoint\n",
        "    report_to=\"none\"                      # no integrated logging to WandB or others\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/biogpt_lora_model\"\n",
        "\n",
        "# Save the LoRA-adapted model (this saves the LoRA adapters and configuration).\n",
        "trainer.model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(f\"Model saved to: {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "HEprZYBXx7d1",
        "outputId": "039a2ef2-0895-4d98-ddc0-67642987d50f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6749' max='7400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6749/7400 5:33:50 < 32:12, 0.34 it/s, Epoch 1.82/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.854100</td>\n",
              "      <td>1.741544</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7400' max='7400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7400/7400 6:06:33, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.854100</td>\n",
              "      <td>1.741544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.799400</td>\n",
              "      <td>1.690355</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: /content/drive/MyDrive/biogpt_lora_model\n",
            "Model saved to: /content/drive/MyDrive/biogpt_lora_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from peft import PeftModel\n",
        "\n",
        "# # Reload the base model and apply the LoRA adapters (if starting fresh, e.g., in a new session):\n",
        "# base_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
        "# model = PeftModel.from_pretrained(base_model, \"radiology_report_lora\")\n",
        "# model.eval()  # set to evaluation mode\n",
        "\n",
        "# # Ensure tokenizer is loaded (if not already from previous steps)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "# if tokenizer.pad_token_id is None:\n",
        "#     tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# # Define a sample prompt (e.g., simulated diagnosis confidence scores for a chest X-ray)\n",
        "# sample_prompt = \"Cardiomegaly: 0; Consolidation: 1; Atelectasis: 0\"\n",
        "\n",
        "# # Tokenize the prompt and generate a report\n",
        "# input_ids = tokenizer(sample_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "# # Generate the completion (report). Limit the length of generation to avoid run-on.\n",
        "# # We'll use greedy decoding for simplicity; for more varied text, one could use beam search or sampling.\n",
        "# outputs = model.generate(input_ids, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
        "# generated_ids = outputs[0][input_ids.shape[-1]:]  # slice out the generated portion (exclude prompt tokens)\n",
        "# generated_report = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "# print(f\"Prompt:\\n{sample_prompt}\\n\")\n",
        "# print(f\"Generated Report:\\n{generated_report}\")\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Paths\n",
        "base_model_name = \"microsoft/biogpt\"\n",
        "lora_path = \"/content/drive/MyDrive/biogpt_lora_model\"\n",
        "\n",
        "# Load base model in float16 instead of 8-bit\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load LoRA adapter\n",
        "model = PeftModel.from_pretrained(base_model, lora_path)\n",
        "model.eval()\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Example prompt\n",
        "sample_prompt = \"Cardiomegaly: 0; Consolidation: 1; Atelectasis: 0\"\n",
        "\n",
        "# Tokenize and generate\n",
        "input_ids = tokenizer(sample_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=200,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode\n",
        "generated_ids = outputs[0][input_ids.shape[-1]:]\n",
        "generated_report = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "# Output\n",
        "print(f\"Prompt:\\n{sample_prompt}\\n\")\n",
        "print(f\"Generated Report:\\n{generated_report}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X5M-JGC52O5",
        "outputId": "aa8360a9-fbac-4f92-ee44-554d60ff62bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "Cardiomegaly: 0; Consolidation: 1; Atelectasis: 0\n",
            "\n",
            "Generated Report:\n",
            ". There is no focal consolidation, pleural effusion, or pneumothorax. The cardiac silhouette is top normal in size. The mediastinal and hilar contours are normal. The pulmonary vasculature is normal. Impression: No focal consolidation to suggest pneumonia. No evidence of acute cardiopulmonary process. No cardiomegaly. No pulmonary vascular congestion. No focal consolidation to suggest pneumonia. No large pleural effusion. No acute cardiopulmonary process. No evidence of cardiomegaly. No evidence of pulmonary vascular congestion. No evidence of acute cardiopulmonary process. No evidence of cardiomegaly. No evidence of pulmonary vascular congestion. No evidence of acute cardiopulmonary process. No evidence of cardiomegaly. No evidence of pulmonary vascular congestion. No evidence of pulmonary vascular engorgement. No evidence of acute cardiopulmonary process. No evidence of cardiomegaly. No evidence of pulmonary vascular engorgement. No evidence of pulmonary vascular engorgement. No evidence of pulmonary vascular engorgement. No evidence of pulmonary vascular engorgement. No evidence of pulmonary vascular engorg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from peft import PeftModel\n",
        "\n",
        "# # Reload the base model and apply the LoRA adapters (if starting fresh, e.g., in a new session):\n",
        "# base_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
        "# model = PeftModel.from_pretrained(base_model, \"radiology_report_lora\")\n",
        "# model.eval()  # set to evaluation mode\n",
        "\n",
        "# # Ensure tokenizer is loaded (if not already from previous steps)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "# if tokenizer.pad_token_id is None:\n",
        "#     tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# # Define a sample prompt (e.g., simulated diagnosis confidence scores for a chest X-ray)\n",
        "# sample_prompt = \"Cardiomegaly: 0; Consolidation: 1; Atelectasis: 0\"\n",
        "\n",
        "# # Tokenize the prompt and generate a report\n",
        "# input_ids = tokenizer(sample_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "# # Generate the completion (report). Limit the length of generation to avoid run-on.\n",
        "# # We'll use greedy decoding for simplicity; for more varied text, one could use beam search or sampling.\n",
        "# outputs = model.generate(input_ids, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
        "# generated_ids = outputs[0][input_ids.shape[-1]:]  # slice out the generated portion (exclude prompt tokens)\n",
        "# generated_report = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "# print(f\"Prompt:\\n{sample_prompt}\\n\")\n",
        "# print(f\"Generated Report:\\n{generated_report}\")\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Paths\n",
        "base_model_name = \"microsoft/biogpt\"\n",
        "lora_path = \"/content/drive/MyDrive/biogpt_lora_model\"\n",
        "\n",
        "# Load base model in float16 instead of 8-bit\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load LoRA adapter\n",
        "model = PeftModel.from_pretrained(base_model, lora_path)\n",
        "model.eval()\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Example prompt\n",
        "sample_prompt = \"Cardiomegaly: 1; Consolidation: 1\"\n",
        "\n",
        "# Tokenize and generate\n",
        "input_ids = tokenizer(sample_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=200,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode\n",
        "generated_ids = outputs[0][input_ids.shape[-1]:]\n",
        "generated_report = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "# Output\n",
        "print(f\"Prompt:\\n{sample_prompt}\\n\")\n",
        "print(f\"Generated Report:\\n{generated_report}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nDjIEI2N0gMZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}